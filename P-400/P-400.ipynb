{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plagarism Detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------------------------------Supervised By:--------------------------------------------------\n",
    "### -----------------------------------------------------------Ranit Devnath Akash -----------------------------------------------------------\n",
    "### ----------------------------------------------------------Lecturer, Dept. of CSE, ----------------------------------------------------------\n",
    "### ----------------------------------------------------Metropolitan University,Sylhet.------------------------------------------------------\n",
    "## ------------------------------------------------Word ta Buligesi:--------------------------------------------------\n",
    "### --------------------Bisahl Shyam Pukayastha-----------------------------Name:Burhan Uddin----------------------------------\n",
    "### --------------------ID: 152-115-010--------------------------------------------ID:153-115-019-------------------------------------------\n",
    "### --------------------CSE 36th Batch,MU--------------------------------------CSE 37th Batch,MU-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import codecs\n",
    "import os\n",
    "import json\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from nltk.stem.snowball import SnowballStemmer\n",
    "from bengali_stemmer.rafikamal2014 import RafiStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'অন্ধকার'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem_word('অন্ধকারে')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### [Important Link] C:\\Users\\Burhan\\AppData\\Roaming\\nltk_data\\corpora\\stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #For English Language\n",
    "stopword = set(stopwords.words('english'))\n",
    "punctuation = set(stopwords.words('punctuation.txt'))\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['এই ', 'ও ', 'থেকে', 'করে', 'এ', 'না', 'ওই', 'এক্', 'নিয়ে', 'করা', 'বলেন ', 'সঙ্গে ', 'যে', 'এব', 'তা', 'আর ', 'কোনো', 'বলে', 'সেই', 'দিন', 'হয় ', 'কি', 'দু', 'পরে', 'সব', 'দেওয়া', 'মধ্যে', 'এর', 'সি', 'শুরু ', 'কাজ', 'কিছু ', 'কাছে', 'সে', 'তবে', 'বা', 'বন', 'আগে', 'জ্নজন', 'পি', 'পর', 'তো', 'ছিল', 'এখন', 'আমরা', 'প্রায়', 'দুই ', 'আমাদের', 'তাই', 'অন্য', 'গিয়ে', 'প্রযন্ত', 'মনে', 'নতুন', 'মতো', 'কেখা', 'প্রথম ', 'আজ', 'টি ', 'ধামার', 'অনেক', 'বিভিন্ন ', 'র ', 'হাজার', 'জানা', 'নয়', 'অবশ্য', 'বেশি', 'এস', 'করে', 'কে', 'হতে', 'বি', 'কয়েক', 'সহ  ', 'বেশ', 'এমন', 'এমনি', 'কেন', 'কেউ', 'নেওয়া', 'চেষ্টা', 'লক্ষ  ', 'বলা', 'কারণ ', 'আছে', 'শুধু ', 'তখন', 'যা', 'এসে', 'চার', 'ছিল', 'যদি', 'আবার', 'কোটি ', 'উত্তর', 'সামনে', 'উপর', 'বক্তব্য ', 'এত', 'প্রাথমিক', 'উপরে', 'আছে', 'প্রতি', 'কাজে', 'যখন', 'খুব', 'বহু', 'গেল', 'পেয়্র্', 'চালু', 'ই ', 'নাগাদ', 'থাকা', 'পাচ', 'যাওয়া', 'রকম', 'সাধারণ', 'কমনে']\n",
      "['.', ',', '?', ';', \"'\", '\"', ':', '(', ')', '!', '@', '#', '—', '$', '%', '^', '&', '*', '_', '+', '=', '{', '}', '[', ']', '\\\\', '|', '/', '<', '>', '~', '`', '।', '।', '—']\n"
     ]
    }
   ],
   "source": [
    "#stopword = set(stopwords.words('bangla.txt'))\n",
    "#print(stopword)\n",
    "x = codecs.open('C:/Users/Burhan/Desktop/P-400/bangla.txt',encoding='utf-8')\n",
    "stopword1 = x.read()\n",
    "stopword1 = stopword1.replace('\\ufeff','')\n",
    "stopword1 = stopword1.split('\\r\\n')\n",
    "print(stopword1)\n",
    "#punctuation = set(stopwords.words('punctuation.txt'))\n",
    "\n",
    "x = codecs.open('C:/Users/Burhan/Desktop/P-400/punctuation.txt',encoding='utf-8')\n",
    "punctuation = x.read()\n",
    "punctuation = punctuation.replace('\\ufeff','')\n",
    "punctuation = punctuation.split('\\r\\n')\n",
    "print(punctuation)\n",
    "\n",
    "stemmer = RafiStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synonyms\n",
    "#synonyms\n",
    "x = codecs.open('C:/Users/Burhan/Desktop/P-400/synonyms.txt', encoding='utf-8')\n",
    "synonyms = x.read()\n",
    "x.close()\n",
    "synonyms = synonyms.split('\\r\\n')\n",
    "synonyms[0] = synonyms[0].replace('\\ufeff','')\n",
    "\n",
    "synonyms_dataset = []\n",
    "for s in synonyms:\n",
    "    synonyms_dataset.append(s.split(','))\n",
    "#synonyms_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL THE CUSTOM FUCTIONS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STOP_WORD_REMOVAL(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STOP_WORD_REMOVAL(token):\n",
    "    filtered_tokens = []\n",
    "    for w in token:\n",
    "        if w not in stopword1:\n",
    "            if w not in punctuation:\n",
    "                filtered_tokens.append(w)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synonyms_Removal(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Synonyms_Removal(token):\n",
    "  #  print(token)\n",
    "  #  print('----------------------------------------------------------')\n",
    "    synonyms_removed_words = []\n",
    "    for t in token:\n",
    "        found = False\n",
    "        for sd in synonyms_dataset:\n",
    "            if t in sd:\n",
    "                synonyms_removed_words.append(sd[0])\n",
    "                found = True\n",
    "                continue\n",
    "        if not found:\n",
    "            synonyms_removed_words.append(t)\n",
    "                \n",
    "   # print(synonyms_removed_words)\n",
    "   # print('__________________________________________________________')\n",
    "    return synonyms_removed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check_Synonyms_Word(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Check_Synonyms_Word(word):\n",
    "    for sd in synonyms_dataset:\n",
    "        if word in sd:\n",
    "            print('FOUND!!!')\n",
    "            print(word+'==='+sd[0])\n",
    "            return\n",
    "    print('NOT FOUND!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N_GRAM_CONVERSION(stage3,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_GRAM_CONVERSION(stage3,n):\n",
    "    #n = 2 #n-grams size\n",
    "    n_grams = []\n",
    "    index = 0\n",
    "    while index <= len(stage3)-n:\n",
    "        s = \"\"\n",
    "        for x in range(n):\n",
    "            s += stage3[index+x]\n",
    "            if x<n-1:\n",
    "                s += ' '\n",
    "        n_grams.append(s)\n",
    "        index = index + 1\n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CREATE_TRIE(index,N_gram,dictionary,documentID,count_N_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CREATE_TRIE(index,N_gram,dictionary,documentID,count_N_gram):\n",
    "   # print(index,N_gram,dictionary,documentID,count_N_gram)\n",
    "    if index < len(N_gram)-1:\n",
    "        if N_gram[index] not in dictionary:\n",
    "            print(N_gram[index]+' not in dictionary')\n",
    "            dictionary[N_gram[index]] = {}\n",
    "        CREATE_TRIE(index+1,N_gram,dictionary[N_gram[index]],documentID,count_N_gram)\n",
    "    else:\n",
    "        if N_gram[index] not in dictionary:\n",
    "            dictionary[N_gram[index]] = {'TOTAL':count_N_gram, documentID:count_N_gram}\n",
    "        else:\n",
    "            dictionary[N_gram[index]]['TOTAL'] += count_N_gram\n",
    "            if documentID not in dictionary:\n",
    "                dictionary[N_gram[index]][documentID] = 0\n",
    "            dictionary[N_gram[index]][documentID] += count_N_gram\n",
    "     #   print(dictionary)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PUNCTUATION(documents):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PUNCTUATION(documents):\n",
    "    for p in punctuation:\n",
    "         documents = documents.replace(p, ' ')\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize(document):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenize(document):\n",
    "    return word_tokenize(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WORD_STEAMING(word):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WORD_STEAMING(word):\n",
    "    temp = stemmer.stem_word(word)\n",
    "    if temp == word:\n",
    "        return temp\n",
    "    else:\n",
    "        return WORD_STEAMING(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEAMING_LIST_CONVERTER(S_list_words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def STEAMING_LIST_CONVERTER(S_list_words):\n",
    "    temp_List = []\n",
    "    for word in S_list_words:\n",
    "        temp_List.append(WORD_STEAMING(word))\n",
    "    return temp_List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding New Files In DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[WinError 5] Access is denied: 'C:/Users/Burhan/Desktop/P-400/Dataset/Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-93a0d99b4d48>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mcorpus\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# for saving memory we can delete our dataset once we load it\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 5] Access is denied: 'C:/Users/Burhan/Desktop/P-400/Dataset/Y'"
     ]
    }
   ],
   "source": [
    "# Version 3.0\n",
    "PATH = 'C:/Users/Burhan/Desktop/P-400/Dataset'\n",
    "Files = os.listdir(PATH)\n",
    "corpus = {}\n",
    "for f in Files:\n",
    "    if f.endswith(\".txt\"):\n",
    "        x = codecs.open(PATH+'/'+f, encoding='utf-8')\n",
    "        #corpus.append(x.read()) for list we change it now on dict to track the file name\n",
    "        corpus[f] = x.read()\n",
    "        x.close()\n",
    "    os.remove(PATH+'/'+f)  # for saving memory we can delete our dataset once we load it\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuationing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in corpus:\n",
    "    corpus[c] = corpus[c].replace('\\ufeff','')\n",
    "    corpus[c] = corpus[c].replace('।', '.')\n",
    "    corpus[c] = PUNCTUATION(corpus[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Flower.txt': ['Daisy',\n",
       "  'flower',\n",
       "  'loves',\n",
       "  'the',\n",
       "  'rose',\n",
       "  'jasmins',\n",
       "  'is',\n",
       "  'another',\n",
       "  'beautiful',\n",
       "  'flower'],\n",
       " 'Fruits.txt': ['monkey', 'loves', 'banana']}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = {}\n",
    "for c in corpus:\n",
    "    tokens[c] = Tokenize(corpus[c])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens_list = {}\n",
    "for t in tokens:\n",
    "    swr = STOP_WORD_REMOVAL(tokens[t])\n",
    "    if len(swr)>0:                    # Removing Empty List\n",
    "        #filtered_tokens_list.append(swr)\n",
    "        filtered_tokens_list[t] = swr\n",
    "#filtered_tokens_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "steamed_token_list = {}\n",
    "stage2 = []\n",
    "for flt in filtered_tokens_list:\n",
    "   # print(flt)\n",
    "    steamed_token_list[flt] = STEAMING_LIST_CONVERTER(filtered_tokens_list[flt])\n",
    "#steamed_token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['অন্ধক']\n"
     ]
    }
   ],
   "source": [
    "print(STEAMING_LIST_CONVERTER(['অন্ধকারে']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before Converting into n-grams lets check synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_removed_steamed_token_list = {}\n",
    "for stl in steamed_token_list:\n",
    "    #synonyms_removed_steamed_token_list.append(Synonyms_Removal(steamed_token_list[stl]))\n",
    "    synonyms_removed_steamed_token_list[stl] = Synonyms_Removal(steamed_token_list[stl])\n",
    "#print(len(steamed_token_list))\n",
    "#print(len(synonyms_removed_steamed_token_list))\n",
    "#synonyms_removed_steamed_token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3: Conversion into n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_grams_list = {}\n",
    "n = 2\n",
    "totalWord = n-1\n",
    "for stl in synonyms_removed_steamed_token_list:\n",
    "    #N_grams_list.append(N_GRAM_CONVERSION(stl,2))\n",
    "    N_grams_list [stl] = N_GRAM_CONVERSION(synonyms_removed_steamed_token_list[stl],n)\n",
    "    totalWord += len(N_grams_list [stl])\n",
    "#N_grams_list\n",
    "#totalWord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams List:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordSets = {}\n",
    "for Ngl in N_grams_list:\n",
    "    wordSets[Ngl] = dict.fromkeys( N_grams_list[Ngl], 0)\n",
    "#wordSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Flower.txt': {'Daisy flower': 2,\n",
       "  'flower loves': 2,\n",
       "  'loves the': 2,\n",
       "  'the rose': 2,\n",
       "  'rose jasmins': 2,\n",
       "  'jasmins is': 2,\n",
       "  'is another': 2,\n",
       "  'another beautiful': 2,\n",
       "  'beautiful flower': 2},\n",
       " 'Fruits.txt': {'monkey loves': 2, 'loves banana': 2}}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for wS in wordSets:\n",
    "    for ngl in N_grams_list[wS]:\n",
    "        wordSets[wS][ngl] +=1\n",
    "wordSets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READ/WRITE OUR TRIE DATASET(JSON):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DICT = {}    # its gonna load from a json file later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### READ FROM TRIE(JSON):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'D': {'a': {'i': {'s': {'y': {' ': {'f': {'l': {'o': {'w': {'e': {'r': {'TOTAL': 2,\n",
       "             'Flower.txt': 2}}}}}}}}}}}},\n",
       " 'f': {'l': {'o': {'w': {'e': {'r': {' ': {'l': {'o': {'v': {'e': {'s': {'TOTAL': 2,\n",
       "             'Flower.txt': 2}}}}}}}}}}}},\n",
       " 'l': {'o': {'v': {'e': {'s': {' ': {'t': {'h': {'e': {'TOTAL': 2,\n",
       "          'Flower.txt': 2}}},\n",
       "       'b': {'a': {'n': {'a': {'n': {'a': {'TOTAL': 2,\n",
       "             'Fruits.txt': 2}}}}}}}}}}}},\n",
       " 't': {'h': {'e': {' ': {'r': {'o': {'s': {'e': {'TOTAL': 2,\n",
       "         'Flower.txt': 2}}}}}}}},\n",
       " 'r': {'o': {'s': {'e': {' ': {'j': {'a': {'s': {'m': {'i': {'n': {'s': {'TOTAL': 2,\n",
       "             'Flower.txt': 2}}}}}}}}}}}},\n",
       " 'j': {'a': {'s': {'m': {'i': {'n': {'s': {' ': {'i': {'s': {'TOTAL': 2,\n",
       "           'Flower.txt': 2}}}}}}}}}},\n",
       " 'i': {'s': {' ': {'a': {'n': {'o': {'t': {'h': {'e': {'r': {'TOTAL': 2,\n",
       "           'Flower.txt': 2}}}}}}}}}},\n",
       " 'a': {'n': {'o': {'t': {'h': {'e': {'r': {' ': {'b': {'e': {'a': {'u': {'t': {'i': {'f': {'u': {'l': {'TOTAL': 2,\n",
       "                  'Flower.txt': 2}}}}}}}}}}}}}}}}},\n",
       " 'b': {'e': {'a': {'u': {'t': {'i': {'f': {'u': {'l': {' ': {'f': {'l': {'o': {'w': {'e': {'r': {'TOTAL': 2,\n",
       "                 'Flower.txt': 2}}}}}}}}}}}}}}}},\n",
       " 'm': {'o': {'n': {'k': {'e': {'y': {' ': {'l': {'o': {'v': {'e': {'s': {'TOTAL': 2,\n",
       "             'Fruits.txt': 2}}}}}}}}}}}}}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json1_file = open('data.json')\n",
    "TEST_DATA_SET_0 = json1_file.read()\n",
    "DATASET_DICT = json.loads(TEST_DATA_SET_0)\n",
    "DATASET_DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WRITE TO  TRIE(JSON):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D not in dictionary\n",
      "a not in dictionary\n",
      "i not in dictionary\n",
      "s not in dictionary\n",
      "y not in dictionary\n",
      "  not in dictionary\n",
      "f not in dictionary\n",
      "l not in dictionary\n",
      "o not in dictionary\n",
      "w not in dictionary\n",
      "e not in dictionary\n",
      "f not in dictionary\n",
      "l not in dictionary\n",
      "o not in dictionary\n",
      "w not in dictionary\n",
      "e not in dictionary\n",
      "r not in dictionary\n",
      "  not in dictionary\n",
      "l not in dictionary\n",
      "o not in dictionary\n",
      "v not in dictionary\n",
      "e not in dictionary\n",
      "l not in dictionary\n",
      "o not in dictionary\n",
      "v not in dictionary\n",
      "e not in dictionary\n",
      "s not in dictionary\n",
      "  not in dictionary\n",
      "t not in dictionary\n",
      "h not in dictionary\n",
      "t not in dictionary\n",
      "h not in dictionary\n",
      "e not in dictionary\n",
      "  not in dictionary\n",
      "r not in dictionary\n",
      "o not in dictionary\n",
      "s not in dictionary\n",
      "r not in dictionary\n",
      "o not in dictionary\n",
      "s not in dictionary\n",
      "e not in dictionary\n",
      "  not in dictionary\n",
      "j not in dictionary\n",
      "a not in dictionary\n",
      "s not in dictionary\n",
      "m not in dictionary\n",
      "i not in dictionary\n",
      "n not in dictionary\n",
      "j not in dictionary\n",
      "a not in dictionary\n",
      "s not in dictionary\n",
      "m not in dictionary\n",
      "i not in dictionary\n",
      "n not in dictionary\n",
      "s not in dictionary\n",
      "  not in dictionary\n",
      "i not in dictionary\n",
      "i not in dictionary\n",
      "s not in dictionary\n",
      "  not in dictionary\n",
      "a not in dictionary\n",
      "n not in dictionary\n",
      "o not in dictionary\n",
      "t not in dictionary\n",
      "h not in dictionary\n",
      "e not in dictionary\n",
      "a not in dictionary\n",
      "n not in dictionary\n",
      "o not in dictionary\n",
      "t not in dictionary\n",
      "h not in dictionary\n",
      "e not in dictionary\n",
      "r not in dictionary\n",
      "  not in dictionary\n",
      "b not in dictionary\n",
      "e not in dictionary\n",
      "a not in dictionary\n",
      "u not in dictionary\n",
      "t not in dictionary\n",
      "i not in dictionary\n",
      "f not in dictionary\n",
      "u not in dictionary\n",
      "b not in dictionary\n",
      "e not in dictionary\n",
      "a not in dictionary\n",
      "u not in dictionary\n",
      "t not in dictionary\n",
      "i not in dictionary\n",
      "f not in dictionary\n",
      "u not in dictionary\n",
      "l not in dictionary\n",
      "  not in dictionary\n",
      "f not in dictionary\n",
      "l not in dictionary\n",
      "o not in dictionary\n",
      "w not in dictionary\n",
      "e not in dictionary\n",
      "m not in dictionary\n",
      "o not in dictionary\n",
      "n not in dictionary\n",
      "k not in dictionary\n",
      "e not in dictionary\n",
      "y not in dictionary\n",
      "  not in dictionary\n",
      "l not in dictionary\n",
      "o not in dictionary\n",
      "v not in dictionary\n",
      "e not in dictionary\n",
      "b not in dictionary\n",
      "a not in dictionary\n",
      "n not in dictionary\n",
      "a not in dictionary\n",
      "n not in dictionary\n"
     ]
    }
   ],
   "source": [
    "for documentID in wordSets:\n",
    "    i = 0\n",
    "    for N_gram in wordSets[documentID]:\n",
    "        i += 1\n",
    "    #    print(str(i)+'/'+str(len(wordSets[documentID]))+' '+N_gram)\n",
    "        CREATE_TRIE(0,N_gram,DATASET_DICT,documentID,wordSets[documentID][N_gram])\n",
    "#DATASET_DICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(DATASET_DICT, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CHECKING(n,word,DICT):\n",
    "    if n< len(word):\n",
    "        if word[n] in DICT:\n",
    "         #   print(word[n])\n",
    "            return CHECKING(n+1,word,DICT[word[n]])\n",
    "        else:\n",
    "            #print('The N-Gram: \"'+word+'\" is not found in DATASET')\n",
    "            return {}\n",
    "    else:\n",
    "        #print(DICT)\n",
    "        #print('The N-Gram: \"'+word+'\" is  found in DATASET')\n",
    "        return DICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def INPUT():\n",
    "    Input_wordSets = {}\n",
    "    Input = input()\n",
    "    TestInput = Input.replace('\\ufeff','')\n",
    "    for p in punctuation:\n",
    "         TestInput = TestInput.replace(p, ' ')\n",
    "    #print(TestInput)\n",
    "    TestInput = word_tokenize(TestInput)\n",
    "    TestInput = STOP_WORD_REMOVAL(TestInput)\n",
    "\n",
    "    #steamming\n",
    "    Steamed_Test_Input = STEAMING_LIST_CONVERTER(TestInput)\n",
    "\n",
    "    TestInput = N_GRAM_CONVERSION(Synonyms_Removal(Steamed_Test_Input),2) # 2 for 2-gram change in func()\n",
    "    #print(Synonyms_Removal(Steamed_Test_Input))\n",
    "    #TestInput = Synonyms_Removal(stl)\n",
    "\n",
    "    Total_n_grams_words = 0\n",
    "    Uniqe_Word = set(TestInput)\n",
    "    Input_wordSets = dict.fromkeys(Uniqe_Word,0)\n",
    "\n",
    "    for word in TestInput:\n",
    "        Input_wordSets[word] +=1\n",
    "        Total_n_grams_words = Total_n_grams_words + 1\n",
    "    print(Input_wordSets)\n",
    "    print('\\n')\n",
    "    print('TOTAL N-Grams : '+ str(Total_n_grams_words)+'\\n')\n",
    "    Result = RESULT(Input_wordSets)\n",
    "    RESULT_SHEET(Result,Total_n_grams_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Version 3.0\n",
    "def RESULT(Input_wordSets):\n",
    "    Result = {}\n",
    "    for Iw in Input_wordSets:\n",
    "        #print(str(Iw)+\" : \"+str(Input_wordSets[Iw]))\n",
    "        temp = CHECKING(0,Iw,DATASET_DICT)\n",
    "        for a in temp:\n",
    "            if a.endswith(\".txt\"):\n",
    "                if a not in Result:\n",
    "                    Result[a] = 0\n",
    "                if temp[a] > Input_wordSets[Iw]:\n",
    "                    Result[a] += Input_wordSets[Iw]\n",
    "                else:\n",
    "                    Result[a] += temp[a]\n",
    "   # print(Result)\n",
    "    return Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RESULT_SHEET(Result,Total_n_grams_words):\n",
    "    A = []\n",
    "    B = []\n",
    "    C = []\n",
    "    for R in Result:\n",
    "        A.append(R)\n",
    "        B.append(Result[R])\n",
    "        C.append(str((Result[R]/Total_n_grams_words)*100)+'%')\n",
    "    data = {'File Name':A,'Matched N-Grams':B,'Simililarity':C}\n",
    "    df = pd.DataFrame(data,columns=['File Name','Matched N-Grams','Simililarity'])\n",
    "    df.sort_values(by=['Simililarity'], inplace=True)\n",
    "    df.head(5)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND!!!\n",
      "তমঃ===অন্ধক\n"
     ]
    }
   ],
   "source": [
    "Check_Synonyms_Word('তমঃ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "প্রিয় তমঃ আমার অন্ধকার খুব ।  আন্ধার বাস্তবটাকে খুব ভালো দেখতে হয়না।"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raisy flower loves the rose\n",
      "{'the rose': 1, 'loves the': 1, 'Raisy flower': 1, 'flower loves': 1}\n",
      "\n",
      "\n",
      "TOTAL N-Grams : 4\n",
      "\n",
      "    File Name  Matched N-Grams Simililarity\n",
      "0  Flower.txt                3        75.0%\n"
     ]
    }
   ],
   "source": [
    "INPUT()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
